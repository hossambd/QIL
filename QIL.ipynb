{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["eQW1IWDCZuq0","oBjS8L5E_QL_","vUhKeYNGYMkd","nDVZor-M0GG4","Tw0KKrB4np-E","FZxjQXSnw1ig","9G4fOuFTnEFc","jGbexfXTpUB-","E3vjFd9Cm899","eHRzzyhNpjMU"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"8ca1bd2cc0494c28b04d13c3de28c88f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_229d74c721a5462a8fd28c82231d2cb8","IPY_MODEL_474ce2786b3b4f4c81f375c83bb86bd6","IPY_MODEL_a618941cc05f49ea80e9e1e2e16252fe"],"layout":"IPY_MODEL_7edf4da9f3334a2d9cc44ad698546624"}},"229d74c721a5462a8fd28c82231d2cb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85694aca9f8e40418b357728eec978f6","placeholder":"​","style":"IPY_MODEL_5fb820ec9f8943f5963e2a57a232e443","value":"Downloading: 100%"}},"474ce2786b3b4f4c81f375c83bb86bd6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c96f2dc80a5a47018fa65b196f221f60","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c0567b996b18457a942a481443127284","value":481}},"a618941cc05f49ea80e9e1e2e16252fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_201eeaac742147aabcb180d5cf9cc5df","placeholder":"​","style":"IPY_MODEL_6a95ad2c0fe54326bde66d0e9285d8cd","value":" 481/481 [00:00&lt;00:00, 17.8kB/s]"}},"7edf4da9f3334a2d9cc44ad698546624":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85694aca9f8e40418b357728eec978f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fb820ec9f8943f5963e2a57a232e443":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c96f2dc80a5a47018fa65b196f221f60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0567b996b18457a942a481443127284":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"201eeaac742147aabcb180d5cf9cc5df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a95ad2c0fe54326bde66d0e9285d8cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94defba0919742beae41c0901c24b320":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_363425c7ac77497cb67d47a9e3b9725c","IPY_MODEL_fb241be4e3564add852e1ac2a5bab8c7","IPY_MODEL_8ce161269edf405fa42d6e0412583619"],"layout":"IPY_MODEL_d6b87a23e19b48f293040d2587aa7570"}},"363425c7ac77497cb67d47a9e3b9725c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80875ff02b514cc89f2c3bf5169c2787","placeholder":"​","style":"IPY_MODEL_6434d32a65a14dcdbe7ce2f67fbe7370","value":"Downloading: 100%"}},"fb241be4e3564add852e1ac2a5bab8c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e634f4fccf5542cb9df09682bc7f23cd","max":501200538,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d7fa333a14a0496480d3769a5610f2b2","value":501200538}},"8ce161269edf405fa42d6e0412583619":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecc2341611d64136816049ef0a78c7c0","placeholder":"​","style":"IPY_MODEL_5a2ef8dc5fd84b1c92cb0f6236757882","value":" 501M/501M [00:06&lt;00:00, 71.5MB/s]"}},"d6b87a23e19b48f293040d2587aa7570":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80875ff02b514cc89f2c3bf5169c2787":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6434d32a65a14dcdbe7ce2f67fbe7370":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e634f4fccf5542cb9df09682bc7f23cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7fa333a14a0496480d3769a5610f2b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ecc2341611d64136816049ef0a78c7c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a2ef8dc5fd84b1c92cb0f6236757882":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["###Monter Google Drive"],"metadata":{"id":"eQW1IWDCZuq0"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir(\"/content/drive/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KVCMTB0cWBbD","executionInfo":{"status":"ok","timestamp":1673459478610,"user_tz":-60,"elapsed":37543,"user":{"displayName":"HOSSAM BOUDRAA","userId":"09632083453126287718"}},"outputId":"8a351f03-77d4-42a1-b779-c4c80d1643a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### Installation"],"metadata":{"id":"oBjS8L5E_QL_"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X3qWqyZSq9SW","executionInfo":{"status":"ok","timestamp":1673459549630,"user_tz":-60,"elapsed":11352,"user":{"displayName":"HOSSAM BOUDRAA","userId":"09632083453126287718"}},"outputId":"5dc3ce62-babe-4b16-f7c4-8b57bbd40df5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"]}]},{"cell_type":"markdown","source":["### Necessary Imports"],"metadata":{"id":"vUhKeYNGYMkd"}},{"cell_type":"code","source":["import math\n","from scipy import stats\n","import numpy as np\n","from tqdm import tqdm\n","import torch.optim as optim\n","from torch import nn, Tensor\n","from torch.autograd import Variable\n","import torch\n","from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel, RobertaForSequenceClassification"],"metadata":{"id":"CoZP7L1cmHlw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Configuration des chemins"],"metadata":{"id":"nDVZor-M0GG4"}},{"cell_type":"code","source":["train_path='/content/drive/MyDrive/Colab Notebooks/QIL/data/annotated_question_intimacy_data/final_train.txt'#--train_path'\n","val_path='/content/drive/MyDrive/Colab Notebooks/QIL/data/annotated_question_intimacy_data/final_val.txt'#--val_path'\n","test_path='/content/drive/MyDrive/Colab Notebooks/QIL/data/annotated_question_intimacy_data/final_test.txt'#--test_path'\n","\n","model_saving_path='/content/drive/MyDrive/Colab Notebooks/QIL/data/outputs'#--model_saving_path' folder\n","test_saving_path='/content/drive/MyDrive/Colab Notebooks/QIL/data/ooo.txt'#--test_saving_path'"],"metadata":{"id":"JYmRDjT_0Eeh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Configuration du modèl"],"metadata":{"id":"Tw0KKrB4np-E"}},{"cell_type":"code","source":["torch.manual_seed(0)\n","\n","pre_trained_model_name_or_path='roberta-base'#or path savedmodel\n","\n","# Create Model with specified optimizer and loss function\n","##############################################################\n","\n","model = RobertaForSequenceClassification.from_pretrained(pre_trained_model_name_or_path,num_labels=1,output_attentions = False,output_hidden_states = False)\n","\n","\n","model_name='roberta-base' #'--model_name' or roberta-ft\n","\n","if model_name == 'roberta-base':\n","\t\t\tmax_epochs = 30\n","\t\t\tlr = 0.00001\n","\t\t\tbatch_size = 128\n","\t\t\tcuda = True\n","\t\t\tmax_len = 50\n","elif model_name == 'roberta-ft':\n","\t\t\tmax_epochs = 30\n","\t\t\tlr = 0.00001\n","\t\t\tbatch_size = 128\n","\t\t\tcuda = True\n","\t\t\tmax_len = 50\n","else:\n","\t\t\tmax_epochs = 30\n","\t\t\tlr = 0.0001\n","\t\t\tbatch_size = 128\n","\t\t\tcuda = True\n","\t\t\tmax_len = 50\n","          \n","if cuda:\n","    model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":188,"referenced_widgets":["8ca1bd2cc0494c28b04d13c3de28c88f","229d74c721a5462a8fd28c82231d2cb8","474ce2786b3b4f4c81f375c83bb86bd6","a618941cc05f49ea80e9e1e2e16252fe","7edf4da9f3334a2d9cc44ad698546624","85694aca9f8e40418b357728eec978f6","5fb820ec9f8943f5963e2a57a232e443","c96f2dc80a5a47018fa65b196f221f60","c0567b996b18457a942a481443127284","201eeaac742147aabcb180d5cf9cc5df","6a95ad2c0fe54326bde66d0e9285d8cd","94defba0919742beae41c0901c24b320","363425c7ac77497cb67d47a9e3b9725c","fb241be4e3564add852e1ac2a5bab8c7","8ce161269edf405fa42d6e0412583619","d6b87a23e19b48f293040d2587aa7570","80875ff02b514cc89f2c3bf5169c2787","6434d32a65a14dcdbe7ce2f67fbe7370","e634f4fccf5542cb9df09682bc7f23cd","d7fa333a14a0496480d3769a5610f2b2","ecc2341611d64136816049ef0a78c7c0","5a2ef8dc5fd84b1c92cb0f6236757882"]},"id":"QLPLvP-Cnmil","executionInfo":{"status":"ok","timestamp":1673459890931,"user_tz":-60,"elapsed":11521,"user":{"displayName":"HOSSAM BOUDRAA","userId":"09632083453126287718"}},"outputId":"6d2168d1-f412-44e4-e391-aa2e8319ba0f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ca1bd2cc0494c28b04d13c3de28c88f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94defba0919742beae41c0901c24b320"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["### Functions"],"metadata":{"id":"FZxjQXSnw1ig"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4bfttNelyg9"},"outputs":[],"source":["def padding(text, pad, max_len = 50):\n","\n","    return text if len(text) >= max_len else (text + [pad] * (max_len-len(text)))\n","\n","def encode_batch(text, berts, max_len = 50):\n","    tokenizer = berts[0]\n","    t1 = []\n","    for line in text:\n","        t1.append(padding(tokenizer.encode(line,add_special_tokens = True, max_length = max_len,truncation=True),tokenizer.pad_token_id,max_len))\n","    return t1\n","\n","\n","def data_iterator(train_x, train_y, batch_size = 64):\n","    n_batches = math.ceil(len(train_x) / batch_size)\n","    for idx in range(n_batches):\n","        x = train_x[idx *batch_size:(idx+1) * batch_size]\n","        y = train_y[idx *batch_size:(idx+1) * batch_size]\n","        yield x, y\n","        \n","\n","def get_metrics(model, test_x, test_y, tokenizer, test = False, save_path='test_prediction_final.txt'):\n","    all_preds = []\n","    test_iterator = data_iterator(test_x, test_y, batch_size=64)\n","    all_y = []\n","    all_x = []\n","    model.eval()\n","    for x, y in test_iterator:\n","        ids = encode_batch(x, (tokenizer,model), max_len = max_len)\n","        with torch.no_grad():\n","            if cuda:\n","                input_ids = Tensor(ids).cuda().long()                \n","                labels = torch.cuda.FloatTensor(y)\n","            else:\n","                input_ids = Tensor(ids).long()\n","                labels = torch.FloatTensor(y)\n","            outputs = model(input_ids, labels=labels)\n","            loss, y_pred = outputs[:2]\n","\n","        predicted = y_pred.cpu().data\n","        all_preds.extend(predicted.numpy())\n","        all_y.extend(y)\n","        all_x.extend(x)\n","\n","    all_res = np.array(all_preds).flatten()\n","    if test and save_path:\n","        with open(save_path, 'w') as w:\n","            for i in range(len(all_y)):\n","                if i < 2:\n","                    print(all_x[i], all_res[i], test_y[i])\n","                w.writelines(all_x[i] + '\\t' + str(all_y[i]) + '\\t' + str(all_res[i]) + '\\n')\n","\n","    #print('pearson r:', stats.pearsonr(all_res, all_y)[0])\n","    score = 0\n","    return loss,stats.pearsonr(all_res, all_y)[0]\n","\n","def run_epoch(model, train_data, val_data, tokenizer, optimizer):\n","    train_x, train_y = train_data[0], train_data[1]\n","    val_x, val_y = val_data[0], val_data[1]\n","    iterator = data_iterator(train_x, train_y, batch_size)\n","    train_losses = []\n","    val_accuracies = []\n","    losses = []\n","\n","    for i, (x,y) in tqdm(enumerate(iterator),total=int(len(train_x)/batch_size)):\n","        #print('iteration', i)\n","        model.zero_grad()\n","\n","        ids = encode_batch(x, (tokenizer,model), max_len = max_len)\n","\n","\n","        if cuda:\n","            input_ids = Tensor(ids).cuda().long()\n","            labels = torch.cuda.FloatTensor(y)\n","        else:\n","            input_ids = Tensor(ids).long()\n","            labels = torch.FloatTensor(y)\n","\n","        outputs = model(input_ids, labels=labels)\n","        loss, logits = outputs[:2]\n","\n","        loss.backward()\n","        #print('train_loss',loss)\n","        losses.append(loss.data.cpu().numpy())\n","        optimizer.step()\n","\n","        if (i + 1) % 1 == 0:\n","            #print(\"Iter: {}\".format(i))\n","            avg_train_loss = np.mean(losses)\n","            train_losses.append(avg_train_loss)\n","            #print(\"\\tAverage training loss: {:.5f}\".format(avg_train_loss))\n","            losses = []\n","\n","            # Evalute Accuracy on validation set\n","            model.eval()\n","            all_preds = []\n","            val_iterator = data_iterator(val_x, val_y, batch_size)\n","            for x, y in val_iterator:\n","                ids = encode_batch(x, (tokenizer,model), max_len = max_len)\n","                #x = Variable(Tensor(x))\n","\n","                with torch.no_grad():\n","\n","                    if cuda:\n","                        input_ids = Tensor(ids).cuda().long()\n","                        labels = torch.cuda.FloatTensor(y)\n","                    else:\n","                        input_ids = Tensor(ids).long()\n","                        labels = torch.FloatTensor(y)\n","                    outputs = model(input_ids, labels=labels)\n","                    loss, y_pred = outputs[:2]\n","\n","                predicted = y_pred.cpu().data\n","\n","                all_preds.extend(predicted.numpy())\n","\n","            \n","            all_res = np.array(all_preds).flatten()            \n","            score = (np.square(val_y - all_res)).mean()\n","            val_accuracies.append(score)\n","            model.train()\n","\n","    return train_losses, val_accuracies\n","\n","def get_test_result(model, test_x, test_y, tokenizer, save_path, ext_test = False, pure_inference=False):\n","    all_raw = []\n","    all_preds = []\n","    all_y = []\n","    all_x = []\n","    test_iterator = data_iterator(test_x, test_y, batch_size=256)\n","    model.eval()\n","    i = 0\n","    for x, y in test_iterator:\n","        print(str(i * 256) + '/' + str(len(test_x)))\n","        i += 1\n","        #print(x[:5])\n","        ids = encode_batch(x, (tokenizer, model), max_len = max_len)\n","        # x = Variable(Tensor(x))\n","\n","        with torch.no_grad():\n","            if cuda:\n","                input_ids = Tensor(ids).cuda().long()\n","                #labels = torch.cuda.FloatTensor(y)\n","            else:\n","                input_ids = Tensor(ids).long()\n","                #labels = torch.FloatTensor(y)\n","            outputs = model(input_ids)\n","            y_pred = outputs[0]\n","\n","        predicted = y_pred.cpu().data\n","        #predicted = torch.max(y_pred.cpu().data, 1)[1]\n","        all_preds.extend(predicted.numpy())\n","        #all_raw.extend(y_pred.cpu().data.numpy())\n","        all_y.extend(y)\n","        all_x.extend(x)\n","\n","\n","    #all_res = [1 if i[0] > 0 else -1 for i in all_preds]\n","    all_res = np.array(all_preds).flatten()\n","    #all_raw = np.array(all_raw)\n","\n","    if save_path:\n","        with open(save_path, 'w') as w:\n","            if pure_inference:\n","                 for i in range(len(all_y)):                \n","                    print(all_x[i], all_res[i])                    \n","                    w.writelines(all_x[i] + '\\t' + str(all_res[i]) + '\\n')\n","            else:\n","                for i in range(len(all_y)):                \n","                    print(all_x[i], all_res[i], test_y[i])                    \n","                    w.writelines(all_x[i] + '\\t' + str(all_y[i]) + '\\t' + str(all_res[i]) + '\\n')\n","    \n","    if not pure_inference:\n","        print('mse:', (np.square(all_y - all_res)).mean())\n","        print('pearson r:', stats.pearsonr(all_res, all_y)[0])\n","\n","    if ext_test:\n","        print('book pearson r:', stats.pearsonr(all_res[:50], all_y[:50])[0])\n","        print('twitter pearson r:', stats.pearsonr(all_res[50:100], all_y[50:100])[0])\n","        print('movie pearson r:', stats.pearsonr(all_res[100:150], all_y[100:150])[0])\n","\n","    return all_res, all_y"]},{"cell_type":"markdown","source":["\n","### Main"],"metadata":{"id":"9G4fOuFTnEFc"}},{"cell_type":"code","source":["\n","def get_data(path):\n","    print('open:',path)\n","    text = []\n","    y = []\n","    with open(path, 'r') as r:\n","        lines = r.readlines()\n","        for line in lines:\n","            line = line.strip('\\n').split('\\t')\n","            text.append(line[0])\n","            if len(line) == 2:\n","                y.append(float(line[1]))\n","            elif len(line) == 1:\n","                y.append(0.0)\n","            else:\n","                print('error in loading:',path)\n","    return text,y\n","\n","\n","\n","\n","mode = 'train'# or 'internal-test' or 'inference'\n","\n","\n","tokenizer = RobertaTokenizer.from_pretrained(pre_trained_model_name_or_path,num_labels=1,output_attentions = False,output_hidden_states = False)\n","\n"],"metadata":{"id":"btEKNvZhm3tf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Mode training\n"],"metadata":{"id":"jGbexfXTpUB-"}},{"cell_type":"code","source":["train_text,  train_labels = get_data(train_path)\n","val_text,  val_labels = get_data(val_path)\n","test_text,  test_labels = get_data(test_path)\n","\n","train_x = train_text\n","train_y = np.array(train_labels)\n","val_x = val_text\n","val_y = np.array(val_labels)\n","model.train()\n","optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n","##############################################################\n","\n","train_data = [train_x, train_y]\n","val_data = [val_x, val_y]\n","\n","# Get Accuracy of final model\n","test_x = test_text\n","test_y = np.array(test_labels)\n","best_val = 100.0\n","best_test = 100.0\n","best_r = 100\n","\n","for i in range(max_epochs):\n","    print (\"Epoch: {}\".format(i))\n","\n","    train_losses,val_accuracies = run_epoch(model, train_data, val_data, tokenizer, optimizer)\n","    test_acc,test_r = get_metrics(model, test_x, test_y,  tokenizer, test = True, save_path=test_saving_path)\n","    #print('Final Test Accuracy: {:.4f}'.format(test_acc))\n","\n","    print(\"\\tAverage training loss: {:.5f}\".format(np.mean(train_losses)))\n","    print(\"\\tAverage Val MSE: {:.4f}\".format(np.mean(val_accuracies)))\n","    if np.mean(val_accuracies) < best_val:\n","        best_val = np.mean(val_accuracies)\n","        best_test = test_acc\n","        best_r = test_r\n","        if i >= 1 and model_saving_path:\n","            model.save_pretrained(model_saving_path)\n","            tokenizer.save_pretrained(model_saving_path)\n","\n","\n","print('model saved at', model_saving_path)\n","print('best_val_loss:', best_val)\n","print('best_test_loss:',best_test)\n","print('best_test_pearsonr:',best_r)\n","\n","#test_acc = get_metrics(model, test_x, test_y, config, tokenizer, test = True)\n","#if args.model_saving_path:\n","#    model.save_pretrained(args.model_saving_path)\n","#    tokenizer.save_pretrained(args.model_saving_path)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9__9pwHHpTx0","executionInfo":{"status":"ok","timestamp":1673088580456,"user_tz":-60,"elapsed":729174,"user":{"displayName":"HOSSAM BOUDRAA","userId":"09632083453126287718"}},"outputId":"c6e6c807-832b-4b0b-b9a9-1a5701236d5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["open: /content/drive/MyDrive/Colab Notebooks/QIL/data/annotated_question_intimacy_data/final_train.txt\n","open: /content/drive/MyDrive/Colab Notebooks/QIL/data/annotated_question_intimacy_data/final_val.txt\n","open: /content/drive/MyDrive/Colab Notebooks/QIL/data/annotated_question_intimacy_data/final_test.txt\n","Epoch: 0\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.034892585 -0.1302005785547401\n","Can I eat and pee at the same time? -0.047029573 0.08370097250899593\n","\tAverage training loss: 0.10142\n","\tAverage Val MSE: 0.0887\n","Epoch: 1\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:23,  1.54s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.08259615 -0.1302005785547401\n","Can I eat and pee at the same time? -0.07534729 0.08370097250899593\n","\tAverage training loss: 0.07601\n","\tAverage Val MSE: 0.0659\n","Epoch: 2\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:23,  1.55s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.08965588 -0.1302005785547401\n","Can I eat and pee at the same time? -0.046453763 0.08370097250899593\n","\tAverage training loss: 0.05868\n","\tAverage Val MSE: 0.0528\n","Epoch: 3\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.53s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.07097919 -0.1302005785547401\n","Can I eat and pee at the same time? 0.13431898 0.08370097250899593\n","\tAverage training loss: 0.05280\n","\tAverage Val MSE: 0.0437\n","Epoch: 4\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:23,  1.58s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.112675354 -0.1302005785547401\n","Can I eat and pee at the same time? -0.084912635 0.08370097250899593\n","\tAverage training loss: 0.04117\n","\tAverage Val MSE: 0.0327\n","Epoch: 5\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:23,  1.54s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.12188815 -0.1302005785547401\n","Can I eat and pee at the same time? -0.0014807478 0.08370097250899593\n","\tAverage training loss: 0.03832\n","\tAverage Val MSE: 0.0303\n","Epoch: 6\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:23,  1.54s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.13945165 -0.1302005785547401\n","Can I eat and pee at the same time? 0.036564216 0.08370097250899593\n","\tAverage training loss: 0.03724\n","\tAverage Val MSE: 0.0315\n","Epoch: 7\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:23,  1.55s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.07511178 -0.1302005785547401\n","Can I eat and pee at the same time? 0.12514317 0.08370097250899593\n","\tAverage training loss: 0.02892\n","\tAverage Val MSE: 0.0299\n","Epoch: 8\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.52s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.09027055 -0.1302005785547401\n","Can I eat and pee at the same time? 0.081479 0.08370097250899593\n","\tAverage training loss: 0.02773\n","\tAverage Val MSE: 0.0273\n","Epoch: 9\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:23,  1.53s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.14362669 -0.1302005785547401\n","Can I eat and pee at the same time? 0.09967284 0.08370097250899593\n","\tAverage training loss: 0.02740\n","\tAverage Val MSE: 0.0265\n","Epoch: 10\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.52s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.099859685 -0.1302005785547401\n","Can I eat and pee at the same time? 0.1485351 0.08370097250899593\n","\tAverage training loss: 0.02869\n","\tAverage Val MSE: 0.0270\n","Epoch: 11\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.51s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.12964326 -0.1302005785547401\n","Can I eat and pee at the same time? -0.015477753 0.08370097250899593\n","\tAverage training loss: 0.02584\n","\tAverage Val MSE: 0.0274\n","Epoch: 12\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.53s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.13109832 -0.1302005785547401\n","Can I eat and pee at the same time? 0.021403972 0.08370097250899593\n","\tAverage training loss: 0.02322\n","\tAverage Val MSE: 0.0264\n","Epoch: 13\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.51s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.09563973 -0.1302005785547401\n","Can I eat and pee at the same time? 0.06249874 0.08370097250899593\n","\tAverage training loss: 0.02053\n","\tAverage Val MSE: 0.0259\n","Epoch: 14\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.52s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.10857286 -0.1302005785547401\n","Can I eat and pee at the same time? 0.050199203 0.08370097250899593\n","\tAverage training loss: 0.01838\n","\tAverage Val MSE: 0.0261\n","Epoch: 15\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.51s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.16953664 -0.1302005785547401\n","Can I eat and pee at the same time? 0.080884784 0.08370097250899593\n","\tAverage training loss: 0.01848\n","\tAverage Val MSE: 0.0257\n","Epoch: 16\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:23,  1.53s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.07552562 -0.1302005785547401\n","Can I eat and pee at the same time? 0.13450515 0.08370097250899593\n","\tAverage training loss: 0.01751\n","\tAverage Val MSE: 0.0247\n","Epoch: 17\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.52s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.059341118 -0.1302005785547401\n","Can I eat and pee at the same time? 0.08399413 0.08370097250899593\n","\tAverage training loss: 0.01526\n","\tAverage Val MSE: 0.0256\n","Epoch: 18\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.52s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.16039276 -0.1302005785547401\n","Can I eat and pee at the same time? 0.064918034 0.08370097250899593\n","\tAverage training loss: 0.01633\n","\tAverage Val MSE: 0.0269\n","Epoch: 19\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.1171948 -0.1302005785547401\n","Can I eat and pee at the same time? 0.06828214 0.08370097250899593\n","\tAverage training loss: 0.01593\n","\tAverage Val MSE: 0.0265\n","Epoch: 20\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.15280165 -0.1302005785547401\n","Can I eat and pee at the same time? -0.07833003 0.08370097250899593\n","\tAverage training loss: 0.01546\n","\tAverage Val MSE: 0.0269\n","Epoch: 21\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.111739576 -0.1302005785547401\n","Can I eat and pee at the same time? 0.08642207 0.08370097250899593\n","\tAverage training loss: 0.01676\n","\tAverage Val MSE: 0.0281\n","Epoch: 22\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.14594314 -0.1302005785547401\n","Can I eat and pee at the same time? 0.06460566 0.08370097250899593\n","\tAverage training loss: 0.01572\n","\tAverage Val MSE: 0.0263\n","Epoch: 23\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.15494628 -0.1302005785547401\n","Can I eat and pee at the same time? 0.10621295 0.08370097250899593\n","\tAverage training loss: 0.01385\n","\tAverage Val MSE: 0.0256\n","Epoch: 24\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.51s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.17152804 -0.1302005785547401\n","Can I eat and pee at the same time? 0.06535132 0.08370097250899593\n","\tAverage training loss: 0.01366\n","\tAverage Val MSE: 0.0259\n","Epoch: 25\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.1790677 -0.1302005785547401\n","Can I eat and pee at the same time? 0.07447932 0.08370097250899593\n","\tAverage training loss: 0.01269\n","\tAverage Val MSE: 0.0259\n","Epoch: 26\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.18168534 -0.1302005785547401\n","Can I eat and pee at the same time? 0.056612723 0.08370097250899593\n","\tAverage training loss: 0.01176\n","\tAverage Val MSE: 0.0252\n","Epoch: 27\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.51s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.23732258 -0.1302005785547401\n","Can I eat and pee at the same time? -0.032278046 0.08370097250899593\n","\tAverage training loss: 0.01179\n","\tAverage Val MSE: 0.0249\n","Epoch: 28\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.14268327 -0.1302005785547401\n","Can I eat and pee at the same time? 0.034724157 0.08370097250899593\n","\tAverage training loss: 0.01072\n","\tAverage Val MSE: 0.0259\n","Epoch: 29\n"]},{"output_type":"stream","name":"stderr","text":["15it [00:22,  1.51s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Besides medical equipment or technology, what is a common, everyday item that would confuse someone the most from the middle ages? -0.20946378 -0.1302005785547401\n","Can I eat and pee at the same time? -0.032980762 0.08370097250899593\n","\tAverage training loss: 0.01151\n","\tAverage Val MSE: 0.0266\n","model saved at /content/drive/MyDrive/Colab Notebooks/QIL/data/outputs\n","best_val_loss: 0.024680131073151135\n","best_test_loss: tensor(0.0192, device='cuda:0')\n","best_test_pearsonr: 0.8117268784473094\n"]}]},{"cell_type":"markdown","source":["### Mode Internal Test\n","The best model will be saved at `outputs/`\n","\n","after training, to get the score on our annotated test and out-domain set,``` \\\n","--pre_trained_model_name_or_path=outputs \\\n","--predict_data_path=data/annotated_question_intimacy_data/final_external.txt "],"metadata":{"id":"E3vjFd9Cm899"}},{"cell_type":"code","source":["# pre_trained_model_name_or_path='/content/drive/MyDrive/Colab Notebooks/QIL/data/outputs'\n","pre_trained_model_name_or_path='/content/drive/MyDrive/Colab Notebooks/QIL/data/out2/best-model'\n","\n","predict_data_path='/content/drive/MyDrive/Colab Notebooks/QIL/data/annotated_question_intimacy_data/final_external.txt'#--predict_data_path'\n","\n","model = RobertaForSequenceClassification.from_pretrained(pre_trained_model_name_or_path,num_labels=1,output_attentions = False,output_hidden_states = False)\n","# if cuda:\n","model.cuda()\n","\n","val_text,  val_labels = get_data(val_path)\n","test_text,  test_labels = get_data(test_path)\n","final_test_text,final_test_y = get_data(predict_data_path)\n","\n","print('external_test:')\n","test_result, test_score = get_test_result(model, final_test_text, final_test_y, tokenizer,save_path=test_saving_path, ext_test = True)\n","\n","print('val:')\n","test_result, test_score = get_test_result(model, val_text, val_labels, tokenizer,save_path=None, ext_test = False)\n","\n","print('test:')\n","test_result, test_score = get_test_result(model, test_text, test_labels, tokenizer,save_path=None, ext_test = False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uLNpmqSnANu","executionInfo":{"status":"ok","timestamp":1673459911560,"user_tz":-60,"elapsed":7622,"user":{"displayName":"HOSSAM BOUDRAA","userId":"09632083453126287718"}},"outputId":"0275ed91-9374-4a46-b36b-dac7d5b20abb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/Colab Notebooks/QIL/data/out2/best-model were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/QIL/data/out2/best-model and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["open: /content/drive/MyDrive/Colab Notebooks/QIL/data/annotated_question_intimacy_data/final_val.txt\n","open: /content/drive/MyDrive/Colab Notebooks/QIL/data/annotated_question_intimacy_data/final_test.txt\n","open: /content/drive/MyDrive/Colab Notebooks/QIL/data/annotated_question_intimacy_data/final_external.txt\n","external_test:\n","0/150\n","Oh, Mr. Stephens, is there nothing we could do? -0.1535802 -0.05413701329856845\n","what is an intimate friend? -0.15055382 0.20087224199975143\n","How can you endure this kind of life? -0.15656313 0.43400000918507603\n","They are frightened--the little ones? -0.15018514 -0.06120741806706436\n","No! Go now, and take me with you! How should I find my way to London alone, or seek out this man Bradbury, or your mother? -0.14958134 -0.2766953082825539\n","What are you doing that for? -0.15145521 0.015148892275376288\n","Know you not that this is a manse? -0.14613369 -0.4536741581121467\n","But what's the sense in playing him , Chip? -0.1554547 0.15421128819114172\n","Now then, sleepy, what's yer game? -0.15338612 -0.016492682952801214\n","Well, Bat, how are you? -0.15183777 0.16062163539464588\n","I mean--what are you going to be? Aren't you going to enter any profession? -0.14709756 0.25917043906233894\n","for of course I shall go that I may look after Dorothy. Do you two girls think you can find your way back home again? -0.15586248 -0.37989956377251066\n","What is the meaning of this, little father? -0.15050586 0.03132961710808823\n","did you say sad news, Katharine? -0.14832106 0.03154290567569518\n","Before we part, Dwyer, will you tell me how you avoided capture on the day that we scoured the whole country in search of you? -0.14945525 0.07353728404737142\n","Will you be my uncle? -0.14948323 0.20769851250975835\n","How could she describe your mother and your home? -0.15285209 0.29241343890523636\n","Is my opinion to be as wrong as all that? -0.1511687 0.1515377247736981\n","Miss Colchester is a very peculiar girl. What does a calculus of finite differences mean? -0.15529129 -0.6455998883930195\n","Why not invite us to supper? -0.1467064 -0.014406920063799865\n","Any letters for the post, sir? -0.15377925 -0.5311291697967893\n","Has it occurred to you, Nina, that he may care for some one else? -0.13918054 0.4190822074855174\n","What are you fellers goin' to do with your share of the gold? -0.14850189 0.2122093585159758\n","How are all the folks to-day? -0.15094343 0.030613865943148\n","Louis Mortimer, will you get me this or that? -0.15652329 -0.293169225703728\n","Is there anything dear Sir Sampson could take? -0.14745429 -0.2076747006707435\n","What is the story, Solomon? -0.15337218 -0.010714985693885633\n","how did I know it wasn't loaded? -0.15040979 -0.2418223907528888\n","Couldn't your father buy you just one pair? -0.15000638 0.16157774146219014\n","You heard what I said. Where is she ? -0.15278539 -0.1857405833908145\n","Then the front door will be ...? -0.1497412 -0.5267461618129247\n","about the dinner-table on the Thursday night. You laid it? -0.14451608 -0.11352193339296082\n","But are you aware who I am? -0.15736058 -0.020865861247572427\n","Why should I speak when every word I utter you believe, or affect to believe, to be the ravings of a maniac? -0.15288821 0.2084437506362485\n","may I sit next to you, and go up alongside and receive with you? -0.14329053 0.14909903275314818\n","Can you imagine how much worse conditions would be in the world had these masters not lived? -0.14988586 0.02687943608079646\n","But apart from dogma, entire liberty of research is permitted us. Do you wish to become acquainted with the hierarchy of Angels, the virtue of Numbers, the explanation of germs and metamorphoses? -0.13734046 -0.12802590715942364\n","what is the cause of this quarrel between two such great men? -0.15451789 0.19914467012591053\n","But how big are they when they're little? -0.15245846 -0.25541607377229636\n","Yes, sir. Is it typhoid, Mr. Grimshaw? -0.1550309 -0.1350480422049925\n","And now, what's the programme? -0.15283163 -0.6766961633779284\n","Don't you think we should stay here another day so there will be no danger of reopening your wound? -0.14984839 -0.12047905760328287\n","And what is the proper manner? -0.15254465 -0.20443703003920077\n","How do you know we wish you to take one? -0.14829849 0.042446898848522485\n","Was it different from what I'd told you everywhere? -0.14428464 0.04339203465818187\n","What were you looking at? -0.15267298 0.06099059020239793\n","Which is the northeast corner, Billy? -0.14800933 -0.45410358922357524\n","and you don't know who is ill? or what is the matter? -0.16107664 0.11409480755918133\n","Can't the poor guy do anything right? -0.14628173 0.2477019241068425\n","Was not the sea Made for the free-- Land for courts and slaves alone? -0.15107185 -0.16740488205886725\n","What account would you buy? -0.15182874 0.030618929116695316\n","Good (cheap) places to get personalised engraved stem caps? -0.156931 -0.3153726495134612\n","Can we be social friend? -0.1519531 0.16751613469546445\n","If o-care is so great, why should there be any exemptions and why shouldn't congress and all fed employees be on it? -0.15528771 -0.5033389232980273\n","Can vav please come back to atlanta? -0.14686166 -0.06920846254209695\n","Did the all female officials at the uefa super cup do a good job? -0.14511469 -0.21990838941698332\n","Why are there water cuts in pune even after the heavy rainfall this year? -0.14732605 -0.4539223352801088\n","Y am not myslf lately? -0.15400857 0.2171431225679311\n","What is the time in your countrt today? -0.15017873 -0.3784090226501843\n","How can people dislike waggy? -0.1576454 -0.19551853828340976\n","So what is really going on? -0.15587908 0.04966907195914503\n","Can i borrow the car for tx2k? -0.15144649 -0.3156498066590777\n","Do i give off trixie or katya vibes? -0.14977294 -0.20604037398401198\n","Hey do i know you? -0.15145028 -0.10140708365205378\n","Lmao what the registration for ? -0.14479905 -0.10988391887227253\n","Is this like a citizenship test? -0.1469989 0.03709584388118185\n","Yo is it me or js mind fucked everyone involved? -0.15325058 0.1524465224249884\n","Anybody looking for a grapple saug player that is dedicated, has lan xp, and can run tonight? -0.15890053 -0.36129769258456695\n","Look who popped up into the 1808 media house? -0.14566359 -0.4336332195140502\n","If you're a volunteer, do you receive travel reimbursements or other benefits? -0.14687923 -0.11401153990757429\n","Are you having fun reading our tweets? -0.1485218 -0.16247246370615667\n","Guess who we have headlining our remix show this weekend? -0.15431616 -0.2735656880362017\n","When married with me? -0.14878312 0.3459601632849769\n","Wifey you coming to bliss or stadium at all this weekend? -0.14120352 -0.10027689876675255\n","When will this show start? -0.14992732 -0.24943564549626757\n","Season 3 of gavin and stacey is finally on netflix but they took season 1 off? -0.15604615 -0.6297856608398517\n","Is it weird i liked more villains than heroes? -0.14757344 0.19443731539411688\n","Why do no jams format their tweets like this? -0.14962119 -0.3347506954232223\n","Which one of you are responsible for people sounding like s tin can or skrillex in game chat? -0.16156009 -0.2245470605235998\n","I cri what happened to me? -0.1532889 0.39710630432102395\n","Connor what the fuck is your header? -0.15664288 0.02579456228074169\n","Do you have any final thoughts you would like to share? -0.15056556 0.4879621467867804\n","Im not ready for this comeback what can i do? -0.15848246 0.3014326232825994\n","Cleveland where y'all at? -0.15690753 -0.19706980983593855\n","Are they really gonna make spents this offseason? -0.14878953 -0.36944447012127396\n","Are you guys voting? -0.14789376 0.35166742330169504\n","Looking for updates from capitol hill? -0.14670189 -0.46774608935278766\n","Why am i crying now? -0.15617788 0.4752178834509864\n","Have you ever been so sad that physically hurt inside? -0.15045354 0.5331364656671295\n","About to put in a support a creator code who should i put ? -0.14913225 -0.1740425049602098\n","Yeol hair color ? -0.1398423 -0.7748308756449698\n","Why do u have phantom busses that never come yet say due? -0.1493595 -0.33979938116380853\n","Why is tilted a ghost town now? -0.1484954 -0.25341213645294314\n","Where to buy kaya that top ni kisses yung nasa cover? -0.14905432 -0.7012730475625243\n","Nust students do you go for zero nights coz you want to or its peer pressure? -0.1454203 0.16173324541239242\n","Why is biggie showing us ike closing his box when elo and diane are having a conversation? -0.15432811 -0.22099334274728072\n","Omg has anyone else's parent just randomly ambushed them about sex? -0.15075912 0.5322782301709145\n","Any eu players wanna play fortnite? -0.15525354 -0.54674739426081\n","Umm, where the hell is the pa system at adelaide oval? -0.1510511 -0.5640158548196111\n","Is kashmir your father's product? -0.15578917 -0.01775867009355033\n","Fine, Bob. How are you? -0.15211493 0.15952863856505364\n","How'd you know it was me? -0.14629893 0.34148090864425773\n","Can we do that? -0.14804858 0.1646387964642557\n","Hello... Oh, my God, who did that to you? -0.1421208 0.4176652623962876\n","Does V'ger object to the presence of the two carbon units? -0.14434926 0.09256645585342187\n","What are you talking about? -0.15403172 -0.0019874063728070393\n","What's this all about? -0.14783669 0.1233587848046287\n","Would you blow the seal on the emergency hatch so I can come in? -0.14881416 0.030158363133924997\n","What'd you think I was doing out here with these glasses on?  Sunnin' myself? -0.14235118 -0.2592507504597913\n","Yeah Walter, what's your point? -0.15973063 0.2221342446527556\n","Mr. Chekov, anything unusual? -0.14804155 0.17566249089010383\n","What the fuck's going on? -0.15243366 0.18183128076112726\n","And what are we going to do now? -0.15435517 0.4369983342702475\n","My bed, gone! Am I to freeze to death? -0.15339893 0.14758265343158591\n","Do you think she's safe? -0.1465705 0.2804756502136415\n","Do you suppose the saints would have smoked if tobacco had been popular back then? -0.14876755 -0.402503863812492\n","No, I don't think I would. So... then what can I do for you? -0.1554949 -0.15887971254144087\n","How many trips did you make? -0.14784387 -0.3138972238716627\n","Is your brother ready to go? -0.14906499 -0.03172539944812738\n","Do you know what you're going to do? -0.14814393 0.04253506529722003\n","Yeah....? So...Jack...are you stroking that big fat fucking cock of yours? -0.14162457 0.443772353413642\n","How clear is Manchester? -0.15150149 -0.5810730029409841\n","So, what about you? -0.15455788 0.2885971557687157\n","Alright, tell me, what'd you hear? -0.15584162 -0.07501681314964165\n","Sandro ...What's the matter? -0.1481193 0.3065559818105386\n","How much did you pay? -0.14631644 0.16720315059446836\n","Excuse me, but have you been here all day? -0.15487018 -0.007355672737554053\n","What's the dope, Sheriff? -0.14757913 -0.0189641644203861\n","Why are you doing this to me? -0.1490426 0.6099561208852782\n","A dollar seventy-nine for wraps-what's that shit? -0.14614236 -0.6319312922535679\n","Did you give him a full cavity search? -0.14786208 0.32553753398713176\n","Not without Claudia. Where is she? -0.149903 -0.1200200300995313\n","Who'd want to be friends with that bunch of pachuco wannabes? -0.13987064 0.37998281667053035\n","Yeah? How you going to fix me up with Betty Grable? -0.15349701 0.1915939924381353\n","How 'bout this one? -0.1548712 0.0443312433390019\n","What about the husbands? -0.14731786 0.034985627029152574\n","Peter, are you okay? -0.15333527 0.2719746160482847\n","I know, I've seen it myself. Would you give that a try? -0.14988542 0.11865838581523475\n","What was Mr. Marsh's physical condition prior to his death? -0.14770095 0.24507570684998325\n","I've never seen a man who could have taken what he just went through.  He's a cop? -0.15515584 0.24152963369119854\n","But why?  Don't you understand what Keating is saying? -0.15544482 0.14129558917741308\n","Why don't they just identify themselves and tell me what they want? -0.16050571 0.1674464103120799\n","Can you finish on schedule? -0.14566043 0.14107607351299573\n","Well... are you self-conscious about it? -0.14747077 0.3358508589505827\n","Babe Brother, is that my shirt that you're wearing? -0.14887297 -0.03416797537726598\n","What the fuck you looking at? -0.15202138 0.08918355165916299\n","Where'm I goinna get a pizza this time o' night? -0.1493361 -0.47710629889447514\n","What's an audience going to tell you? -0.14617361 0.16800293874929723\n","Can I have that pad and the pencil? -0.14829347 -0.48773400890376856\n","Who have you been talking to? -0.14687034 0.27426410220014424\n","mse: 0.10409865670003393\n","pearson r: 0.04454588528612433\n","book pearson r: 0.1301149029586398\n","twitter pearson r: -0.10725560544198791\n","movie pearson r: 0.042485180706197494\n","val:\n","0/225\n","mse: 0.11903789059058285\n","pearson r: 0.049487107031076005\n","test:\n","0/225\n","mse: 0.11011713395250167\n","pearson r: -0.0048189440295636965\n"]}]},{"cell_type":"markdown","source":["### Mode inference \n","\n","to run the fine-tuned model over your own data, prepare a file with a list of input text like `data/inference.txt` ```\\\n","--predict_data_path=data/inference.txt \\\n","--test_saving_path=ooo.txt"],"metadata":{"id":"eHRzzyhNpjMU"}},{"cell_type":"code","source":["predict_data_path='/content/drive/MyDrive/Colab Notebooks/QIL/data/inference.txt'#--predict_data_path'\n","test_saving_path='/content/drive/MyDrive/Colab Notebooks/QIL/data/ooo.txt'#--test_saving_path'\n","\n","final_test_text,final_test_y = get_data(predict_data_path)\n","test_result, test_score = get_test_result(model, final_test_text, final_test_y, tokenizer,save_path=test_saving_path, ext_test = False, pure_inference=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"756ylg9ZpjWs","executionInfo":{"status":"ok","timestamp":1673459938146,"user_tz":-60,"elapsed":1236,"user":{"displayName":"HOSSAM BOUDRAA","userId":"09632083453126287718"}},"outputId":"6609bc85-c0bf-4222-c773-ea26c718a539"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["open: /content/drive/MyDrive/Colab Notebooks/QIL/data/inference.txt\n","0/3\n","Do you love me? -0.14744307\n","What's your favorite hobby? -0.14598438\n","are you student? -0.14496452\n"]}]}]}